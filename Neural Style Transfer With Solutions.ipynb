{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Style Transfer With Solutions","private_outputs":true,"provenance":[{"file_id":"https://github.com/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb","timestamp":1606093987739}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jo5PziEC4hWs"},"source":["# Neural Style Transfer"]},{"cell_type":"markdown","metadata":{"id":"aDyGj8DmXCJI"},"source":["## Overview\n","\n","In this assignment, you will complete the missing pieces of a TensorFlow implementation of the Neural Style Transfer algorithm, which allows you to apply the style of one image to the content of another.  The algorithm is based on a pretrained convolutional neural network model called VGG-19 [\\[4\\]](#refs), which we will be using to extract low-level and high-level visual features from images.\n","\n","By exploiting the fact that low-level visual features correspond roughly to what we think of as \"style,\" and high-level features correspond roughly to \"content,\" we use these two kinds of features to define a measure of style similarity and content similarity between any two pairs of images.  Then, once we've defined these similarity measures, we can use them to generate images which with style similar to one input image, but content similar to another.  We generate these images based on the iterative optimization process of gradient descent, gradually refining the pixels of a candidate image to achieve the desired style and content.\n"]},{"cell_type":"markdown","metadata":{"id":"Ai3jQzszOIEH"},"source":["# Credit\n","\n","This assignment is adapted from [Raymond Yuan's excellent tutorial on Neural Style Transfer using TensorFlow](https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398) [\\[5\\]](#refs).  Most of the code in this assignment, and some of the explanations, originally appeared in that tutorial.\n","\n","The original tutorial contains no exercises or missing code sections.  Our contributions consist of converting this tutorial to a multi-part course assignment with questions, coding problems, and hints, as well as adding Questions 4 and 5 to allow the student to interactively explore the effect of different hyperparameter choices on the model.  The ideas which are explored in Questions 4 and 5 are based on ideas described in the original paper on Neural Style Transfer by Gatys et al [\\[3\\]](#refs)."]},{"cell_type":"markdown","metadata":{"id":"R7NvwTmhP9Ax"},"source":["## Learning Objectives\n","After this project, you should feel comfortable with the following:\n","\n","* Running pretrained convolutional neural network (CNN) models\n","* Understanding how different layers of a CNN process different features of an image\n","* Using pretrained models to extract features for use in other algorithms\n","* Using latent features to define similarity measurements between images\n","* Understanding the design of the Neural Style Transfer algorithm, including:\n","  * the loss functions it defines\n","  * how it uses gradient descent\n","  * the effect of different choices of hyperparameters\n"]},{"cell_type":"markdown","metadata":{"id":"eqxUicSPUOP6"},"source":["## Setup\n","\n","### Import and configure modules"]},{"cell_type":"code","metadata":{"id":"sc1OLbOWhPCO"},"source":["import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","mpl.rcParams['figure.figsize'] = (10,10)\n","mpl.rcParams['axes.grid'] = False\n","\n","import numpy as np\n","from PIL import Image\n","import time\n","import functools"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYEjlrYk3s6w"},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","\n","from tensorflow.python.keras.preprocessing import image as kp_image\n","from tensorflow.python.keras import models \n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import backend as K"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8ajP_u73s6m"},"source":["### Download Images"]},{"cell_type":"code","metadata":{"id":"riWE_b8k3s6o"},"source":["import os\n","img_dir = '/tmp/nst'\n","if not os.path.exists(img_dir):\n","    os.makedirs(img_dir)\n","\n","!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/d/d7/Green_Sea_Turtle_grazing_seagrass.jpg\n","!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5zn7xsJpoK7"},"source":["# enable eager execution such that \"operations return concrete values instead of constructing a computational graph to run later.\"\n","# https://www.tensorflow.org/guide/eager\n","tf.enable_eager_execution()\n","print(\"Eager execution: {}\".format(tf.executing_eagerly()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IOiGrIV1iERH"},"source":["# Set up some global values here\n","content_path = '/tmp/nst/Green_Sea_Turtle_grazing_seagrass.jpg'\n","style_path = '/tmp/nst/The_Great_Wave_off_Kanagawa.jpg'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xE4Yt8nArTeR"},"source":["## Visualize the input"]},{"cell_type":"code","metadata":{"id":"3TLljcwv5qZs"},"source":["def load_img(path_to_img):\n","  max_dim = 512\n","  img = Image.open(path_to_img)\n","  long = max(img.size)\n","  scale = max_dim/long\n","  img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n","  \n","  img = kp_image.img_to_array(img)\n","  \n","  # We need to broadcast the image array such that it has a batch dimension \n","  img = np.expand_dims(img, axis=0)\n","  return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vupl0CI18aAG"},"source":["def imshow(img, title=None):\n","  # Remove the batch dimension\n","  out = np.squeeze(img, axis=0)\n","  # Normalize for display \n","  out = out.astype('uint8')\n","  plt.imshow(out)\n","  if title is not None:\n","    plt.title(title)\n","  plt.imshow(out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2yAlRzJZrWM3"},"source":["These are input content and style images. We hope to \"create\" an image with the content of our content image, but with the style of the style image. "]},{"cell_type":"code","metadata":{"id":"_UWQmeEaiKkP"},"source":["plt.figure(figsize=(10,10))\n","\n","content = load_img(content_path).astype('uint8')\n","style = load_img(style_path).astype('uint8')\n","\n","plt.subplot(1, 2, 1)\n","imshow(content, 'Content Image')\n","\n","plt.subplot(1, 2, 2)\n","imshow(style, 'Style Image')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7qMVNvEsK-_D"},"source":["## Prepare the data\n","Let's create methods that will allow us to load and preprocess our images easily. We perform the same preprocessing process as are expected according to the VGG training process. VGG networks are trained on image with each channel normalized by `mean = [103.939, 116.779, 123.68]`and with channels BGR."]},{"cell_type":"code","metadata":{"id":"hGwmTwJNmv2a"},"source":["def load_and_process_img(path_to_img):\n","  img = load_img(path_to_img)\n","  img = tf.keras.applications.vgg19.preprocess_input(img)\n","  return img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xCgooqs6tAka"},"source":["In order to view the outputs of our optimization, we are required to perform the inverse preprocessing step. Furthermore, since our optimized image may take its values anywhere between $- \\infty$ and $\\infty$, we must clip to maintain our values from within the 0-255 range.   "]},{"cell_type":"code","metadata":{"id":"mjzlKRQRs_y2"},"source":["def deprocess_img(processed_img):\n","  x = processed_img.copy()\n","  if len(x.shape) == 4:\n","    x = np.squeeze(x, 0)\n","  assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n","                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n","  if len(x.shape) != 3:\n","    raise ValueError(\"Invalid input to deprocessing image\")\n","  \n","  # perform the inverse of the preprocessing step\n","  x[:, :, 0] += 103.939\n","  x[:, :, 1] += 116.779\n","  x[:, :, 2] += 123.68\n","  x = x[:, :, ::-1]\n","\n","  x = np.clip(x, 0, 255).astype('uint8')\n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GEwZ7FlwrjoZ"},"source":["### Choose intermediate layers \n","Choose intermediate layers from the network to represent the style and content of the image:"]},{"cell_type":"code","metadata":{"id":"N4-8eUp_Kc-j"},"source":["#choose intermediate layers\n","content_layers = ['block5_conv2'] \n","\n","# Original, recommended 'style_layers' (in case you modify the real ones below):\n","\"\"\"\n","style_layers = ['block1_conv1',\n","                'block2_conv1',\n","                'block3_conv1', \n","                'block4_conv1', \n","                'block5_conv1'\n","               ]\n","\"\"\"\n","\n","# Later(Q5. Effects of Using Different Subsets of CNN Layers) you will try differnt subsets of these \n","# layers. Don't forget to change it back once you finish the corresponding section.\n","style_layers = ['block1_conv1',\n","                'block2_conv1',\n","                'block3_conv1', \n","                'block4_conv1', \n","                'block5_conv1'\n","               ]\n","\n","num_content_layers = len(content_layers)\n","num_style_layers = len(style_layers)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jt3i3RRrJiOX"},"source":["## Q1 Build the Model\n","\n","\n","\n","We want to build a model which allows us to get some feature representation values so that we can use them to measure how similar two images are in terms of their content and style.  We will use the hidden layers of the [VGG19](https://keras.io/applications/#vgg19) model as a source of both content and style features.\n","\n","We will be using the VGG19 model to build a wrapper model which returns the feature maps generated in its hidden layers.  We can construct a TensorFlow model with a given set of input and output layers with the call:\n","`model = Model(inputs, outputs)`"]},{"cell_type":"markdown","metadata":{"id":"rvOjYgiG3A4D"},"source":["**a. Your job is to complete the get_vgg_model().** We need to specify the `model_outputs`, a list of output values.\n","\n","Hint: Use `MODEL.get_layer(LAYER_NAME).output` to get a output layer named `LAYER_NAME` in `MODEL`. Which model are we using? What layers do we want?"]},{"cell_type":"code","metadata":{"id":"nfec6MuMAbPx"},"source":["def get_vgg_model(style_layers, content_layers):\n","  \"\"\" Creates a vgg model that returns a list of intermediate output values. \"\"\"\n","  \n","  # Load pretrained VGG model, trained on imagenet data\n","  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n","  vgg.trainable = False\n","\n","  # Get output layers corresponding to style and content layers\n","  # BEGIN YOUR SOLUTION \n","  style_outputs = [vgg.get_layer(name).output for name in style_layers]\n","  content_outputs = [vgg.get_layer(name).output for name in content_layers]\n","  model_outputs = style_outputs + content_outputs\n","  # END YOUR SOLUTION\n","\n","  # Build model  \n","  return models.Model(vgg.input, model_outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vJdYvJTZ4bdS"},"source":["##Q2 Define and create our loss functions (content and style distances)"]},{"cell_type":"markdown","metadata":{"id":"5z_wJOtqOaf3"},"source":["To generate an image that matches the content of the content image and the style of the style image, We need to define some loss functions for content and style of our images so that we can use optimizer to minimize these losses "]},{"cell_type":"markdown","metadata":{"id":"F2Hcepii7_qh"},"source":["### Content Loss"]},{"cell_type":"markdown","metadata":{"id":"TgC3ZbufIYcX"},"source":["We will define the content loss as the squared Euclidean distance between two content feature maps, viewing each feature of each pixel as a distinct coordinate.  Let $F^l_{ij}(x)$ and $P^l_{ij}(p)$ denote the respective internal feature maps produced by layer $l$ of the network when evaluated with a candidate image $x$ and content source image $p$. The content loss is then defined as: $$L^l_{content}(p, x) = \\sum_{i, j} (F^l_{ij}(x) - P^l_{ij}(p))^2$$ "]},{"cell_type":"markdown","metadata":{"id":"YQ2D3JcqW5_t"},"source":["**a. Your job is to complete `get_content_loss()`.**\n","\n","Hint: Functions you may need: `tf.reduce_mean` and `tf.square`. "]},{"cell_type":"code","metadata":{"id":"d2mf7JwRMkCd"},"source":["def get_content_loss(base_content_feature, target_feature):\n","  \"\"\" Compute the content loss of two feature values.\n","    Arguments:\n","      base_content_feature: feature value of the base content image in some layer.\n","      target_feature: feature value of the target image content in some layer.\n","\n","    Returns the content loss between two input feature maps.\n","  \"\"\"\n","  # BEGIN YOUR SOLUTION \n","  return tf.reduce_mean(tf.square(base_content_feature - target_feature))\n","  # END YOUR SOLUTION"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lGUfttK9F8d5"},"source":["## Style Loss"]},{"cell_type":"markdown","metadata":{"id":"I6XtkGK_YGD1"},"source":["To compute the difference in style between two images, we don't use the direct squared Euclidean distance between feature maps.  Instead, we compute a kind of summary of each image's distribution of style features, and then compare these summaries.  Specifically, we will summarize each image's feature map at a given layer using its *Gram matrix*, which represents the correlation between different features across all pixels.  The Gram matrix for the candidate image is defined as:\n","$$ G^l_{ij} = \\sum_k F^l_{ik} F^l_{jk} $$\n","where the indices $i, j$ range over feature layers, and the index $k$ ranges over pixels.  The Gram matrix $A$ for the style source image is defined analogously.  The style loss is then defined as\n","$$ E_l = \\frac{1}{4 N_l^2 M_l^2} \\sum_{i,j} (G^l_{ij} - A^l_{ij})^2 $$\n","(Hint: the normalizing factor $N_l^2 M_l^2$ there is just to make the equation into a mean instead of a sum.  You won't need to include it explicitly in your code if you use the right function)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F21Hm61yLKk5"},"source":["### Computing style loss\n"]},{"cell_type":"markdown","metadata":{"id":"QfoIVu6aXCvR"},"source":["**b. Your job is to complete `get_style_loss()`.** You have done something similar in last question."]},{"cell_type":"code","metadata":{"id":"N7MOqwKLLke8"},"source":["def gram_matrix(input_tensor):\n","  \"\"\" Returns the gram matrix of the input tensor.\"\"\"\n","  # We make the image channels first \n","  channels = int(input_tensor.shape[-1])\n","  a = tf.reshape(input_tensor, [-1, channels])\n","  n = tf.shape(a)[0]\n","  gram = tf.matmul(a, a, transpose_a=True)\n","  return gram / tf.cast(n, tf.float32)\n","\n","def get_style_loss(base_style, gram_target):\n","  \"\"\"Expects two images of dimension h, w, c\"\"\"\n","  # height, width, num filters of each layer\n","  # We scale the loss at a given layer by the size of the feature map and the number of filters\n","  height, width, channels = base_style.get_shape().as_list()\n","  \n","  gram_style = gram_matrix(base_style)\n","  \n","  # BEGIN YOUR SOLUTION\n","  return tf.reduce_mean(tf.square(gram_style - gram_target))\n","  # END YOUR SOLUTION"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pXIUX6czZABh"},"source":["## Q3 Apply style transfer to our images\n"]},{"cell_type":"markdown","metadata":{"id":"y9r8Lyjb_m0u"},"source":["### Run Gradient Descent \n","\n","Neural Style Transfer works by iteratively adjusting a *candidate output image* using gradient descent to minimize the style and content losses.  Note that this is different from many common applications of gradient descent in ML: we are updating an image, *not* the weights of any neural network!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-kGzV6LTp4CU"},"source":["We have defined a helper function `get_feature_representations` to compute our content and style feature representations for you. "]},{"cell_type":"code","metadata":{"id":"O-lj5LxgtmnI"},"source":["def get_feature_representations(model, content_path, style_path):\n","  \"\"\"Helper function to compute our content and style feature representations.\n","\n","  This function will simply load and preprocess both the content and style \n","  images from their path. Then it will feed them through the network to obtain\n","  the outputs of the intermediate layers. \n","  \n","  Arguments:\n","    model: an image classifier model\n","    content_path: The path to the content image.\n","    style_path: The path to the style image\n","    \n","  Returns:\n","    returns the style features and the content features. \n","  \"\"\"\n","  # Load images\n","  content_image = load_and_process_img(content_path)\n","  style_image = load_and_process_img(style_path)\n","  \n","  # batch compute content and style features\n","  style_outputs = model(style_image)\n","  content_outputs = model(content_image)\n","  \n","  \n","  # Get the style and content feature representations from our model  \n","  style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n","  content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\n","  return style_features, content_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3DopXw7-lFHa"},"source":["### Computing the loss and gradients\n"]},{"cell_type":"markdown","metadata":{"id":"Pj2ZJBOe6O9R"},"source":["We compute the total loss by accumulating content loss and style loss from each layer. We equally weight each contribution of each loss layer. However, before adding both losses to the final total loss, we weight the total content loss and total style loss differently. That's why we add the argument `loss_weights`, a pair of weights for content loss and style loss to the `compute_loss` function. We will explore more about this parameter in the \"Content/Style Ratio Effects\" section."]},{"cell_type":"markdown","metadata":{"id":"u5Lxkn_iDDX5"},"source":["**a. Your job is to complete `compute_loss()`.**\n","\n","Hint: We have provided an example of how to do it for style loss before the part you need to implement. You need to do the similar thing for content loss. "]},{"cell_type":"code","metadata":{"id":"oVDhSo8iJunf"},"source":["def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n","  \"\"\"Computes the total loss.\n","  \n","  Arguments:\n","    model: The model that will give us access to the intermediate layers\n","    loss_weights: The weights of each contribution of each loss function. \n","      (style weight, content weight, and total variation weight)\n","    init_image: Our initial base image. This image is what we are updating with \n","      our optimization process. We apply the gradients wrt the loss we are \n","      calculating to this image.\n","    gram_style_features: Precomputed gram matrices corresponding to the \n","      defined style layers of interest.\n","    content_features: Precomputed outputs from defined content layers of \n","      interest.\n","      \n","  Returns:\n","    returns the total loss, style loss, content loss, and total variational loss\n","  \"\"\"\n","  style_weight, content_weight = loss_weights\n","  \n","  # Feed our init image through our model. This will give us the content and \n","  # style representations at our desired layers. Since we're using eager execution\n","  # our model is callable just like any other function!\n","  model_outputs = model(init_image)\n","  \n","  style_output_features = model_outputs[:num_style_layers]\n","  content_output_features = model_outputs[num_style_layers:]\n","  \n","  style_score = 0\n","  content_score = 0\n","\n","  # Accumulate style losses from all layers\n","  weight_per_style_layer = 1.0 / float(num_style_layers)\n","  for target_style, comb_style in zip(gram_style_features, style_output_features):\n","    style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\n","  \n","  # Accumulate content losses from all layers \n","  # BEGIN YOUR SOLUTION\n","  weight_per_content_layer = 1.0 / float(num_content_layers)\n","  for target_content, comb_content in zip(content_features, content_output_features):\n","    content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)\n","  # END YOUR SOLUTION\n","\n","  # weight the total contribution from each type\n","  style_score *= style_weight\n","  content_score *= content_weight\n","\n","  # Get total loss\n","  total_loss = style_score + content_score \n","  return total_loss, style_score, content_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5XTvbP6nJQa"},"source":["Then we want to compute the gradient. We apply the gradients of the loss we are calculating to optimize the initial base image.\n","\n","We use TensorFlow's [**tf.GradientTape**](https://www.tensorflow.org/programmers_guide/eager#computing_gradients) feature to compute the gradient.  You don't need to understand much about how this works, but you should know that it puts TensorFlow in a mode where it automatically tracks the gradients of tensors that you compute with respect to the parameters you're trying to optimize."]},{"cell_type":"markdown","metadata":{"id":"VMtDPXq6CQMk"},"source":["**b. Your job is to complete `compute_grads_and_total_loss()`**\n","\n","Hint: Below is an example taken from the documentation of [**tf.GradientTape**](https://www.tensorflow.org/programmers_guide/eager#computing_gradients). Try to do some pattern matching. Remember we are updating the base image(`cfg['init_image']`) instead of some other weights.\n","\n","```\n","w = tf.Variable([[1.0]])\n","with tf.GradientTape() as tape:\n","  loss = w * w\n","\n","grad = tape.gradient(loss, w)\n","```"]},{"cell_type":"code","metadata":{"id":"fwzYeOqOUH9_"},"source":["def compute_grads_and_total_loss(cfg):\n","  \"\"\"Computes the gradients.\n","  Arguments: \n","  cfg: a dictionary of configuration variables passed in compute_loss()\n","  Returns: gradent object, \n","  \"\"\"\n","  \n","  with tf.GradientTape() as tape: \n","    all_loss = compute_loss(**cfg)\n","  total_loss = all_loss[0]\n","\n","  # BEGIN YOUR SOLUTION\n","  w = cfg['init_image']\n","  grad = tape.gradient(total_loss, w)\n","  # END YOUR SOLUTION\n","  return grad, all_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T9yKu2PLlBIE"},"source":["### Optimization loop\n","\n","Now we put everything together to define our `run_style_transfer()` function. The function takes the paths to a content image and a style image, lists of layer names we use for feature extraction(we specified the layers in the cell right before the 'Build the Model' section) and the relative weights for content and style losses and output the best image and best loss after doing some optimization over the loss.\n","We will use Adaptive Moment Estimation(Adam), a gradient descent optimization algorithm, optimizer to minimize the loss. You are not required to undertand how Adam works, but you should be able to follow the instructions to figure out how and where to use it."]},{"cell_type":"markdown","metadata":{"id":"dZTTiFeocnvu"},"source":["**c. Your job is to complete run_style_transfer().**\n","\n","Hint: Here are a few things you need to do:\n","\n","\n","1.   You need to create an optimizer using `tf.train.AdamOptimizer` with `learning_rate=5`, `beta1=0.99` and `epsilon=1e-1`.\n","2.   You need to use this optimizer to apply the processed gradients with [**`apply_gradients()`**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#apply_gradients). Note that `apply_gradients()` takes a list of tuples. In our case, we only need to provide one tuple: (grads, variable_we_try_to_update). Which variable are we trying to apply the gradients to, or what do we need to update?\n","3.   You need to add an if statement to compare the losses before updating the `best_loss` and `best_image`. We are trying to minimize the loss to get the best image. To update the best_image, you can just use `deprocess_img(init_image.numpy())`.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"pj_enNo6tACQ"},"source":["import IPython.display\n","\n","def run_style_transfer(content_path, \n","                       style_path,\n","                       content_layers,\n","                       style_layers,\n","                       num_iterations=1000,\n","                       content_weight=1e3, \n","                       style_weight=1e-2,\n","                       show_images=False): \n","  # We don't need to (or want to) train any layers of our model, so we set their\n","  # trainable to false. \n","  model = get_vgg_model(content_layers, style_layers) \n","  for layer in model.layers:\n","    layer.trainable = False\n","  \n","  # Get the style and content feature representations (from our specified intermediate layers) \n","  style_features, content_features = get_feature_representations(model, content_path, style_path)\n","  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n","  \n","  # Set initial image\n","  init_image = load_and_process_img(content_path)\n","  init_image = tf.Variable(init_image, dtype=tf.float32)\n","\n","  # Create our optimizer\n","  # BEGIN YOUR SOLUTION\n","  optimizer = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n","  # END YOUR SOLUTION\n","\n","  # For displaying intermediate images \n","  iter_count = 1\n","  \n","  # Store our best result\n","  best_loss, best_img = float('inf'), None\n","  \n","  # Create a nice config \n","  loss_weights = (style_weight, content_weight)\n","  cfg = {\n","      'model': model,\n","      'loss_weights': loss_weights,\n","      'init_image': init_image,\n","      'gram_style_features': gram_style_features,\n","      'content_features': content_features\n","  }\n","    \n","  # For displaying\n","  num_rows = 2\n","  num_cols = 5\n","  display_interval = num_iterations/(num_rows*num_cols)\n","  start_time = time.time()\n","  global_start = time.time()\n","  \n","  norm_means = np.array([103.939, 116.779, 123.68])\n","  min_vals = -norm_means\n","  max_vals = 255 - norm_means   \n","  \n","  imgs = []\n","\n","  # iteratively update the base image based on the loss\n","  for i in range(num_iterations):\n","    grads, all_losses = compute_grads_and_total_loss(cfg)\n","    total_loss, style_score, content_score = all_losses\n","    # Apply the gradient\n","    # BEGIN YOUR SOLUTION\n","    optimizer.apply_gradients([(grads, init_image)])\n","    # END YOUR SOLUTION\n","    clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n","    init_image.assign(clipped)\n","    end_time = time.time() \n","    \n","    # Update best loss and best image from total loss.\n","    # BEGIN YOUR SOLUTION\n","    if total_loss < best_loss: \n","      best_loss = total_loss\n","      best_img = deprocess_img(init_image.numpy())\n","    # END YOUR SOLUTION\n","\n","\n","    if i % display_interval== 0:\n","      start_time = time.time()\n","      \n","      # Use the .numpy() method to get the concrete numpy array\n","      plot_img = init_image.numpy()\n","      plot_img = deprocess_img(plot_img)\n","      imgs.append(plot_img)\n","      IPython.display.clear_output(wait=True)\n","      if show_images:\n","        IPython.display.display_png(Image.fromarray(plot_img))\n","      print('Iteration: {}'.format(i))        \n","      print('Total loss: {:.4e}, ' \n","            'style loss: {:.4e}, '\n","            'content loss: {:.4e}, '\n","            'time: {:.4f}s'.format(total_loss, style_score, content_score, time.time() - start_time))\n","  print('Completed! Total time: {:.4f}s'.format(time.time() - global_start))\n","\n","  # Display all intermediate output images at the specified iteration interval \n","  if show_images:\n","    IPython.display.clear_output(wait=True)\n","    plt.figure(figsize=(14,4))\n","    for i,img in enumerate(imgs):\n","        plt.subplot(num_rows,num_cols,i+1)\n","        plt.imshow(img)\n","        plt.xticks([])\n","        plt.yticks([])\n","      \n","  return best_img, best_loss "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSVMx4burydi"},"source":["# It may take 2-3 minutes to run this function\n","best_img, best_loss = run_style_transfer(content_path, \n","                                     style_path, content_layers, style_layers, show_images=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dzJTObpsO3TZ"},"source":["Image.fromarray(best_img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LwiZfCW0AZwt"},"source":["## Visualize outputs\n","We \"deprocess\" the output image in order to remove the processing that was applied to it. "]},{"cell_type":"code","metadata":{"id":"lqTQN1PjulV9"},"source":["def show_results(best_img, content_path, style_path, show_large_final=True):\n","  plt.figure(figsize=(10, 5))\n","  content = load_img(content_path) \n","  style = load_img(style_path)\n","\n","  plt.subplot(1, 2, 1)\n","  imshow(content, 'Content Image')\n","\n","  plt.subplot(1, 2, 2)\n","  imshow(style, 'Style Image')\n","\n","  if show_large_final: \n","    plt.figure(figsize=(10, 10))\n","\n","    plt.imshow(best_img)\n","    plt.title('Output Image')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6d6O50Yvs6a"},"source":["show_results(best_img, content_path, style_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VEgEVwRMMehI"},"source":["## Q4 Content/Style Ratio Effects"]},{"cell_type":"markdown","metadata":{"id":"E0sFWTsIfTpu"},"source":["**Your job is to run the following cell and answer the following question.**\n","\n","Question: Any noticible difference you found from the following output images? What causes this difference?"]},{"cell_type":"markdown","metadata":{"id":"Wb9vDD66gJhE"},"source":["**BEGIN YOUR SOLUTION**\n","\n","The image(s) with higher content/style ratio has emphasis on content while the one(s) with lower content/style ratio has emphasis on style. This is caused by the different relative weightings between the content and style reconstruction. \n","\n","**END YOUR SOLUTION**"]},{"cell_type":"code","metadata":{"id":"qYExk5G-m589"},"source":["# Depending on how many pairs of weights you try, running this cell may take \"number of pairs\" * 3 mins/pair\n","content_style_weights = [(1e3, 1e4),(1e3, 1e2),(1e3, 1e-2),(1e5, 1e-2),(1e7, 1e-2)]\n","plt.figure(figsize=(20, 10))\n","\n","for i, weights in enumerate(content_style_weights):\n","  c_w = weights[0]\n","  s_w = weights[1]\n","  print('content/style: ' + str(c_w/s_w))\n","\n","  best_img, best_loss = run_style_transfer(content_path, \n","                       style_path,\n","                       content_layers,\n","                       style_layers,\n","                       content_weight=c_w, \n","                       style_weight=s_w)\n","  \n","\n","\n","  plt.subplot(len(content_style_weights), 1, i+1)\n","  plt.imshow(best_img)\n","  plt.title('content/style: ' + str(c_w/s_w))\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ulA_xIlHL48a"},"source":["## Q5 Effects of Using Different Subsets of CNN Layers"]},{"cell_type":"markdown","metadata":{"id":"O2-wxeSCKrpV"},"source":["Now we want to exmaine the effects of using different subsets of CNN layers for style."]},{"cell_type":"markdown","metadata":{"id":"K1ZhVtYVKE9N"},"source":["**a. Your job is to try some combinations of layers in the predefined variable `style_layer`in the section \"Choose intermediate layers\" and run the code in the following cell. Then describe what you observed. Make sure you rerun the functions from the section\"Build the Model\" up to and including `run_style_transfer()` function because they depend on the variable you changed.**\n","\n","Hint: Doing this with increasing subsets in order may help you understand the effects easier."]},{"cell_type":"markdown","metadata":{"id":"xzyzc42XMoKv"},"source":["**BEGIN YOUR SOLUTION**\n","\n","The image structures captured by the style representation increase in size and complexity when including style features from higher layers of the network.\n","\n","**END YOUR SOLUTION**"]},{"cell_type":"code","metadata":{"id":"6KdKNljYm7QE"},"source":["best_img, best_loss = run_style_transfer(content_path, \n","                                     style_path, content_layers, style_layers, show_images=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tyGMmWh2Pss8"},"source":["## Q6 Try it on other images\n","\n","Now you know how to do the neural style transfer! "]},{"cell_type":"markdown","metadata":{"id":"p_gMXZVxeQjC"},"source":["**a. Your job is to try this technique on other images and download your favorite one to submit**. \n","\n","To download an image from Colab, you may find the following code in comments useful."]},{"cell_type":"code","metadata":{"id":"GN4U54jxdfDK"},"source":["#from google.colab import files\n","#files.download('artwork.png')\n","\n","# BEGIN YOUR SOLUTION\n","# END YOUR SOLUTION"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lemy0_XNcov9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U-y02GWonqnD"},"source":["<a name='refs'></a>\n","\n","# References\n","\n","\\[1\\] **[Image of Tuebingen](https://commons.wikimedia.org/wiki/File:Tuebingen_Neckarfront.jpg)** \n","Photo By: Andreas Praefcke [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC BY 3.0  (https://creativecommons.org/licenses/by/3.0)], from Wikimedia Commons\n","\n","\\[2\\] **[Image of Green Sea Turtle](https://commons.wikimedia.org/wiki/File:Green_Sea_Turtle_grazing_seagrass.jpg)**\n","By P.Lindgren [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)], from Wikimedia Commons\n","\n","\\[3\\] Gatys, L. A., Ecker, A. S., Bethge, M. et al. \"A neural algorithm of artistic style.\" arXiv preprint, 2015, https://arxiv.org/abs/1508.06576.\n","\n","\\[4\\] Simonyan, K., Zisserman, A. \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\" arXiv preprint, 2015, https://arxiv.org/pdf/1409.1556.\n","\n","\\[5\\] Yuan, R. \"Neural Style Transfer: Creating Art with Deep Learning using tf.keras and eager execution.\" Medium, 3, Aug. 2018, https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398.\n","\n","\\[6\\] \"Neural style transfer.\" TensorFlow.org, 2020, https://www.tensorflow.org/tutorials/generative/style_transfer.\n","\n"]},{"cell_type":"code","metadata":{"id":"IpUD9W6ZkeyM"},"source":[""],"execution_count":null,"outputs":[]}]}
